{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Construindo um agente de inteligência artificial do zero**\n",
        "\n",
        "*Um guia simples e prático para construir a sua IA autônoma com LangGraph*\n"
      ],
      "metadata": {
        "id": "DuoTegBFwCzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://github.com/PatriciaLucas/Minicurso_Agentes_CBIC2025/blob/main/figuras/RAG.png?raw=true' alt=\"drawing\" width=\"800\" />\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gdZN5FfK6VST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introdução:** O Advento dos Agentes de IA\n",
        "\n",
        "  No campo da Inteligência Artificial (IA), um agente é uma entidade artificial projetada para perceber seu ambiente por meio de sensores, tomar decisões com base nessas percepções e executar ações de forma autônoma para alcançar seu(s) objetivo(s). A autonomia, nesse contexto, refere-se à capacidade do agente de operar sem intervenção humana direta, selecionando e realizando ações adequadas diante de diversas situações que possam surgir. Com o surgimento e crescimento dos grandes modelos de linguagem (LLMs), a construção desses agentes tornou-se não apenas mais acessível, como também extremamente poderosa para inúmeras aplicações.\n",
        "\n",
        "  Perante esse novo cenário de demanda crescente de implementações de agentes para diferentes finalidades, esse texto visa conduzir o leitor através de conceitos e ferramentas essenciais para a construções de agentes inteligentes, permitindo que você crie as suas próprias aplicações sofisticadas.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "TVkVZSoRwuht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain e LangGraph:** As Ferramentas de Construção dos Agentes\n",
        "\n",
        "Esse tópico é designado para detalhar as duas ferramentas mais importantes para a construção de agentes: **LangChain** e **LangGraph**.\n",
        "\n",
        "LangChain é um framework projetado para a construção de aplicações em Python que são baseadas em LLMs. Na prática, ele oferece ferramentas para simplificar o desenvolvimento de aplicações de conversação mais complexas e interativas.\n",
        "\n",
        "Um dos principais componentes do LangChain são os Chat Models, que são modelos de linguagem adaptados especificamente para conversação. Eles possuem funcionalidades extras que permitem a integração com ferramentas externas e a capacidade de trabalhar com entradas e saídas de dados estruturadas.\n",
        "\n",
        "A comunicação com esses modelos é feita através de uma lista de mensagens, que podem ser dos seguintes tipos:\n",
        "\n",
        "\n",
        "*   **SystemMessage**: Representa as instruções de contexto e as regras gerais que modelo deve seguir. É utilizada para definir o comportamento do modelo, como o estilo da conversa, a linguagem a ser utilizada e quaisquer limitações que ele deva ter.o\n",
        "\n",
        "*  **HumanMessage**: Representa a entrada do usuário humano. É o que o modelo interpreta como uma pergunta, um comando ou um dado de entrada para processar.\n",
        "\n",
        "*  **AIMessage**: Representa a resposta gerada pelo modelo de IA.\n",
        "\n"
      ],
      "metadata": {
        "id": "cObjh9pnwwu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Potencialização dos agentes:** Retrieval Augmented Generation (RAG)\n",
        "A geração aumentada por generalização é uma das técnicas mais importantes para dar conhecimento externo ao seu agente. O RAG aprimora as respostas da sua LLM, permitindo que ela consulte uma base externa de conhecimento, atualizada, que funciona como uma memória de longo prazo. Isso resolve o paradigma de limitação do conhecimento das LLMs ao seu treinamento, que acaba deixando os dados estáticos e desatualizados.\n",
        "\n",
        "O processo do RAG é dividido em duas etapas principais: **indexação** e **recuperação**. Na indexação, documentos externos são carregados, divididos em pedaços menores (chunks), transformados em vetores numéricos (embeddings) que capturam seu significado e armazenados em um banco de dados vetorial (vector store). Quando o usuário faz uma pergunta, a etapa de recuperação busca os chunks mais relevantes nesse banco de dados e os insere no prompt junto à pergunta original.\n",
        "\n",
        "Para aprofundar seus conhecimentos sobre o tema, consulte o tutorial oficial da LangChain sobre RAG: https://python.langchain.com/docs/tutorials/rag.\n"
      ],
      "metadata": {
        "id": "zVZHRxvVwxgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview do grafo e escolha da LLM\n",
        "\n",
        "Um grafo de controle é o mapa mental do sistema de agentes. A partir dele é definido como o fluxo de pensamento e as ações do agente devem acontecer a partir do prompt que o usuário envia. Esses grafos são direcionados, possuindo um vértice inicial (start) e um vértice final (end). Se quiser entender melhor sobre grafos, acesse esse [link](https://pt.wikipedia.org/wiki/Teoria_dos_grafos). O grafo a seguir representa o fluxo de raciocínio dos agentes neste projeto.\n",
        "\n",
        "\n",
        "<img src='https://github.com/PatriciaLucas/Minicurso_Agentes_CBIC2025/blob/main/figuras/grafo_assistente.png?raw=true' alt=\"drawing\" width=\"300\" />"
      ],
      "metadata": {
        "id": "Ub1e1TqVhiKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando o agente entra em operação, ele parte do nó inicial e segue para o assistente. Nesse ponto, uma LLM previamente definida recebe o prompt e o interpreta.\n",
        "\n",
        "Em seguida, o fluxo pode passar por dois **nós intermediários**:\n",
        "\n",
        "1. O primeiro é o *tools*, que representa o conjunto de ferramentas externas que o agente pode acionar para auxiliar na resposta.\n",
        "2. O segundo é o *moderador*, onde outra instância de LLM verifica se a resposta gerada é adequada ao problema e, se necessário, fornece feedback ao assistente principal.\n",
        "\n",
        "**Dependendo da entrada**, o assistente pode seguir para um desses nós ou encerrar o fluxo de pensamento diretamente. Esse grafo permite visualizar, em tempo real, como o agente está raciocinando e em qual estágio do processo ele se encontra.\n",
        "\n",
        "A escolha da LLM é livre, mas no caso do nó assistente, ela precisa oferecer suporte a chamadas de ferramentas (tool calling), para que o agente possa interagir com o nó tools quando necessário. Além disso, o poder da LLM deve estar alinhado à complexidade do problema: **quanto mais exigente a tarefa, mais robusto deve ser o modelo**, como ChatGPT, Gemini ou outros equivalentes.\n",
        "\n",
        "O processo de inicialização do agente, juntamente com a definição de qual LLM será utilizada, será exibida em código logo abaixo."
      ],
      "metadata": {
        "id": "CTYSBtSPmCWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definindo o modelo de LLM que será utilizado e alguns parâmetros úteis.\n",
        "model = ChatDeepInfra(\n",
        "            model= \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
        "            temperature=0,       # configura o modelo com amostragem determinística\n",
        "            max_tokens = 256,    # número máximo de tokens que o modelo pode gerar\n",
        "            deepinfra_api_token=API_KEY\n",
        "        )\n",
        "\n",
        "# a partir do modelo de LLM, inicializamos um agente específico para a tarefa de\n",
        "# ser o assistente.\n",
        "agent_assistente = initialize_agent(\n",
        "    tools, # ferramentas externas que ele terá acesso.\n",
        "    model,\n",
        "    agent=\"zero-shot-react-description\",  # usa a técnica ReAct.\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# o agente moderador é uma simples instância da LLM escolhida.\n",
        "agent_moderador = model"
      ],
      "metadata": {
        "id": "EoL98CfNn7Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste projeto, utilizamos a infraestrutura da DeepInfra para acessar as LLMs, mas o processo é muito semelhante ao de outros provedores.\n",
        "\n",
        "Após inicializar o modelo, o próximo passo é criar o agente assistente, **integrando a LLM às suas tools e ao modelo de comportamento [ReAct](https://arxiv.org/abs/2210.03629)**. Esse modelo combina o raciocínio em cadeia (chain of thought) com a execução de ações, permitindo que o agente pense sobre o problema e use suas ferramentas de forma integrada e inteligente.\n",
        "\n",
        "Também é criado o agente moderador, como uma simples instância do modelo LLM escolhido. Isso é feito, pois esse agente não necessita de ferramentas externas nem de um padrão de comportamento específico como o ReAct."
      ],
      "metadata": {
        "id": "GDhzOBGqowDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tools\n",
        "Como mencionado anteriormente, as tools são ferramentas externas à LLM que o agente pode acessar para buscar informações específicas ou executar tarefas especializadas.\n",
        "\n",
        "Uma tool pode ser, por exemplo, uma função que conecta o agente a um serviço externo — como uma API de previsão do tempo, a API da NASA para obter dados astronômicos em tempo real, ou até uma função responsável por realizar um cálculo específico.\n",
        "\n",
        "É um recurso bastante versátil, e neste projeto utilizamos uma única tool, chamada **retrieve_context**, responsável por fazer a recuperação de informações do vector store. Observe a implementação dela logo abaixo."
      ],
      "metadata": {
        "id": "looCVHJGpphv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação de uma função responsável pelo RAG\n",
        "def rag(documento):\n",
        "    global retriever\n",
        "\n",
        "    # Exemplo com URL\n",
        "    urls = [documento]\n",
        "\n",
        "    # Load documentos\n",
        "    loader = UnstructuredURLLoader(urls=urls)\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Split documentos\n",
        "    text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"],\n",
        "                                                   chunk_size=200,\n",
        "                                                   chunk_overlap=20)\n",
        "    doc_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Criação VectorStore\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=doc_splits,\n",
        "        collection_name=\"docs\",\n",
        "        embedding = DeepInfraEmbeddings(model_id=\"BAAI/bge-base-en-v1.5\", deepinfra_api_token=API_KEY), # Chamada do modelo de embedding\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Instancia a VectorStore para buscar por k nos documentos internos\n",
        "    # que estejam relacionados com o tema requerido no prompt\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "# Criação da Tool responsável pelo RAG\n",
        "@tool\n",
        "def retrieve_context(query: str):\n",
        "    \"\"\"Pesquise notícias recentes sobre astronomia.\"\"\"\n",
        "    global retriever\n",
        "    results = retriever.invoke(query) # Chama a VectorStore para pesquisar por documentos com base na query\n",
        "    print(results)\n",
        "    return \"\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "# Adiciona a ferramenta na lista de tools que é fornecida para o agente\n",
        "tools = [retrieve_context]"
      ],
      "metadata": {
        "id": "GwnvZTR3qG6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repare que o RAG é o responsável por todo o processo de ingestão e preparação dos dados: ele absorve os documentos, os converte em vetores e os armazena em uma vector store.\n",
        "\n",
        "A tool atua sobre esse conjunto, **buscando dentro dos documentos as informações relevantes**. Neste caso, buscaremos tópicos sobre astronomia.\n",
        "\n",
        "Quando o agente opta por acionar essa ferramenta, ele escolhe uma palavra chave para fazer a consulta. Assim, internamente é chamado o método invoke a partir do retriever, uma instância da VectorStore encarregada de recuperar os dados vetorizados.\n",
        "\n",
        "Dessa forma, sempre que o agente precisa pesquisar notícias recentes sobre astronomia, ele aciona essa tool, terceirizando a busca e garantindo uma resposta mais precisa e contextualizada."
      ],
      "metadata": {
        "id": "KAlhqBtWrfZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#State\n",
        "No LangGraph, o State é uma estrutura de dados compartilhada entre todos os nós do grafo.\n",
        "\n",
        "Ele funciona como uma memória transitória, armazenando o contexto da execução e as informações relevantes. Isso permite que cada nó leia, modifique e repasse esses dados ao longo do fluxo.\n",
        "\n",
        "O state do nosso grafo foi definido da seguinte forma:"
      ],
      "metadata": {
        "id": "Q61rdkW1sazA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "    mensagens: List[BaseMessage]\n",
        "    documento: str\n",
        "    avaliacao: str\n",
        "    feedback: str"
      ],
      "metadata": {
        "id": "6ojbCU_Vtf9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repare que, no código, o State é uma classe. No nosso caso, temos os seguintes atributos:\n",
        "\n",
        "* **mensagens:** reúnem todo o histórico de mensagens, desde os\n",
        "\n",
        "comandos do sistema até as interações do usuário e as respostas geradas.\n",
        "\n",
        "* **documento:** representa o conteúdo carregado no RAG.\n",
        "\n",
        "* **avaliação** indicam se há necessidade de o assistente revisar ou reprocessar sua resposta.\n",
        "\n",
        "* **feedback** mensagem enviada do agente moderador ao agente assistente, informando sobre a correção ou ajuste que deve ser realizado em resposta a uma avaliação negativa."
      ],
      "metadata": {
        "id": "RRMKXma0thZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição dos nós\n",
        "No LangGraph, os nós correspondem a funções que implementam a lógica dos agentes. Cada nó recebe o estado atual como entrada, executa um processamento específico e retorna um estado atualizado.\n",
        "\n",
        "No nosso agente de astronomia será necessário definir 3 nós, sendo eles: nó moderador, nó assistente e nó roteador.  "
      ],
      "metadata": {
        "id": "-wZx8DrUdtfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Nó Assistente\n",
        "Este é o nó que utiliza o agente que será especializado em astronomia, ele tem acesso às ferramentas externas definidas anteriormente que perimitirão construir respostas completas e especializadas em astronomia. Assim como os nós do LangGraph ele recebe um estado e após o processsamento retorna um estado atualizado."
      ],
      "metadata": {
        "id": "emFptnpee_g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nó assitente\n",
        "def assistente(state: State):\n",
        "    #Retem informações do documento\n",
        "    global retriever\n",
        "    retriever = rag(state[\"documento\"])\n",
        "    feedback = state.get(\"feedback\", \"\")\n",
        "\n",
        "    # Pega a última mensagem humana. O caso 2 é para funcionar no langsmith.\n",
        "    last_human_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        # caso 1: já é HumanMessage\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            last_human_message = msg.content\n",
        "            break\n",
        "        # caso 2: veio como dict serializado\n",
        "        if isinstance(msg, dict) and msg.get(\"type\") == \"human\":\n",
        "            last_human_message = msg.get(\"content\")\n",
        "            break\n",
        "\n",
        "    #Definição da SystemMessage para definição do comportamento do agente assistente\n",
        "    prompt_system = f\"\"\"\n",
        "    Você é um assistente de astronomia super gentil que se comunica em português.\n",
        "    \"\"\"\n",
        "    #Definição da HumanMessage: configuração do Prompt Base para geração de resposta\n",
        "    prompt_assistente = f\"\"\"\n",
        "    Responda a Pergunta usando a dica para formular melhor sua resposta.\n",
        "\n",
        "    Pergunta: {last_human_message}\n",
        "    Dica: {feedback}\n",
        "\n",
        "    Responda APENAS em JSON válido no formato:\n",
        "    {{\"resposta\": \"sua resposta aqui\"}}\n",
        "\n",
        "    \"\"\"\n",
        "    #Contrução da lista de mensagens e invocação do agente assistente\n",
        "    messages = [SystemMessage(content=prompt_system), HumanMessage(content=prompt_assistente)]\n",
        "    response = agent_assistente.invoke(messages)\n",
        "\n",
        "    result = json.loads(response['output'])\n",
        "\n",
        "    resposta = result.get(\"resposta\", \"\").strip()\n",
        "\n",
        "    #Atualização do estado com a resposta do agente assistente\n",
        "    state[\"messages\"] = state[\"messages\"] + [AIMessage(content=resposta)]\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "vDqJD8LLgCgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando o nó assistente é executado, o mecanismo de busca RAG é inicializado com base no documento atual, o feedback disponível e a última mensagem humana registrada no histórico são recuperados.\n",
        "\n",
        "Em seguida, um prompt combinando a pergunta do usuário e o feedback fornecido pelo agente moderador é construído. Esse prompt é então enviado ao agente assistente, que retorna uma resposta em formato JSON. O conteúdo gerado é extraído e incorporado ao histórico de mensagens, resultando na atualização do estado da conversa."
      ],
      "metadata": {
        "id": "5_4XDdaiiahk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nó Moderador\n",
        "Este nó implementa o agente moderador que tem a função de avaliar se a resposta produzida pelo agente assistente é adequada e satisfatória. Se a resposta atender aos critérios esperados, o processo é concluído e o usuário a recebe. Caso contrário, o moderador gera um feedback construtivo, que é incorporado ao prompt do agente assistente (como visto anteriormente) para orientar uma nova formulação da resposta."
      ],
      "metadata": {
        "id": "vHiFu6lMAPue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OZ8K8oKMtov"
      },
      "outputs": [],
      "source": [
        "def moderador(state: State):\n",
        "\n",
        "    # Pega a última mensagem AI. O caso 2 é para funcionar no langsmith.\n",
        "    last_ai_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        # caso 1: já é AIMessage\n",
        "        if isinstance(msg, AIMessage):\n",
        "            last_ai_message = msg.content\n",
        "            break\n",
        "        # caso 2: veio como dict serializado\n",
        "        if isinstance(msg, dict) and msg.get(\"type\") == \"ai\":\n",
        "            last_ai_message = msg.get(\"content\")\n",
        "            break\n",
        "\n",
        "    # Pega a última mensagem humana. O caso 2 é para funcionar no langsmith.\n",
        "    last_human_message = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        # caso 1: já é HumanMessage\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            last_human_message = msg.content\n",
        "            break\n",
        "        # caso 2: veio como dict serializado\n",
        "        if isinstance(msg, dict) and msg.get(\"type\") == \"human\":\n",
        "            last_human_message = msg.get(\"content\")\n",
        "            break\n",
        "\n",
        "    # Prompt template\n",
        "    parser = JsonOutputParser()\n",
        "\n",
        "    # Prompt que determina o comportamento do agente moderador e seu objetivo\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Você é um moderador de RESPOSTAS de um agente que só deve falar sobre o tema astronomia.\"),\n",
        "        (\"human\",\n",
        "         \"\"\"Dada a Pergunta e a Resposta, responda se a Resposta está adequada para a Pergunta.\n",
        "         Responda \"sim\" se a resposta estiver adequada e \"não\" se a resposta não estiver adequada.\n",
        "\n",
        "         Se sua resposta for não, dê um feedback curto para que o agente assistente melhore sua resposta.\n",
        "\n",
        "         Responda APENAS em JSON válido no formato:\n",
        "         {{\"resposta\": \"sim\", \"feedback\": \"Você não possui feedback.\"}}\n",
        "         ou\n",
        "         {{\"resposta\": \"não\", \"feedback\": \"Escreva seu feedback aqui\"}}.\n",
        "\n",
        "         Pergunta: {pergunta}\n",
        "         Resposta: {resposta}\"\"\"\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Invocação do agente moderado com tratamento de erro\n",
        "    chain = prompt | agent_moderador | parser\n",
        "    try:\n",
        "        result = chain.invoke({\"pergunta\": last_human_message, \"resposta\": last_ai_message})\n",
        "        resposta = result.get(\"resposta\", \"\").strip()\n",
        "        feedback = result.get(\"feedback\", \"\").strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Erro ao interpretar resposta:\", e)\n",
        "        resposta = \"sim\"\n",
        "        feedback = \"Você não possui feedback.\"\n",
        "\n",
        "    print({\"avaliacao\": resposta,\"feedback\": feedback})\n",
        "    # Atualização do estado com a resposta e feedback do agente assistente\n",
        "    state[\"avaliacao\"] = resposta\n",
        "    state[\"feedback\"] = feedback\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando o nó moderador é executado, a última mensagem enviada pelo assistente e a última pergunta feita pelo usuário são recuperadas. Em seguida, um prompt que instrui o agente a agir como um avaliador é construído, verificando se a resposta do assistente está adequada à pergunta e restrita ao tema de astronomia. O modelo então responde em formato JSON, indicando “sim” ou “não” e, caso necessário, fornece um breve feedback. En seguida, a avaliação e o feedback são atualizados no estado, permitindo que o direcionamento para o fim da conversa ou de volta pata o agente assistente."
      ],
      "metadata": {
        "id": "SUmVwpP7CsJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Nó Roteador\n",
        "Este nó tem como função determinar o próximo passo do fluxo com base na avaliação realizada pelo agente moderador. Esse tipo de nó é comum em arquiteturas desenvolvidas com o LangGraph, nas quais os nós atuam como etapas de decisão ou de processamento dentro do grafo."
      ],
      "metadata": {
        "id": "BgBfVOZYCuTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def roteador(state: State):\n",
        "    \"\"\"Roteia para o agente_assistente ou finaliza.\"\"\"\n",
        "    avaliacao = state.get(\"avaliacao\")\n",
        "    #Caso a avaliação seja negativa, voltamos ao nó assistente\n",
        "    if avaliacao == 'não':\n",
        "            return \"refazer\"\n",
        "\n",
        "    return 'fim'\n"
      ],
      "metadata": {
        "id": "f3G41boRCuuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiramente, recupera-se a avaliacao feita pelo agente moderador, que indica se a resposta anterior foi considerada adequada ou não. Caso o valor seja \"não\", o roteador redireciona o fluxo para \"refazer\", retornando ao agente assistente para que a resposta seja reformulada com base no feedback fornecido. Se a avaliação for positiva, retorna \"fim\", encerrando o processo e permitindo que a resposta final seja enviada ao usuário."
      ],
      "metadata": {
        "id": "jhAuK96SCvNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Criação do Grafo\n",
        "Esse trecho define e organiza todo o fluxo de execução do grafo no LangGraph, estruturando como os nós se conectam entre si e em que ordem serão executados, assim como foi definido e explicado na seção 'Overview do grafo e escolha da LLM'."
      ],
      "metadata": {
        "id": "vWfIGP51DyzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o grafo\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "# Adicionando os nós\n",
        "tool_node = ToolNode(tools=tools)\n",
        "workflow.add_node(\"assistente\", assistente)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "workflow.add_node(\"moderador\", moderador)\n",
        "\n",
        "# Conectando os nós\n",
        "workflow.add_edge(START, \"assistente\")\n",
        "workflow.add_conditional_edges(\"assistente\", tools_condition)\n",
        "workflow.add_edge(\"tools\", \"assistente\")\n",
        "workflow.add_edge(\"assistente\", \"moderador\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"moderador\",\n",
        "    roteador,\n",
        "    {\n",
        "        \"refazer\": \"assistente\",\n",
        "        \"fim\": END\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "MogLbIAeEC9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O grafo é conectado iniciando no nó \"assistente\" (workflow.add_edge(START, \"assistente\")), que pode seguir para o nó \"tools\" conforme a condição definida em tools_condition. O nó \"tools\" retorna ao \"assistente\", permitindo que o agente use ferramentas antes de prosseguir. Em seguida, o fluxo avança para o \"moderador\", responsável por avaliar a resposta gerada.\n",
        "\n",
        "Por fim, a conexão condicional workflow.add_conditional_edges(\"moderador\", roteador, {\"refazer\": \"assistente\", \"fim\": END}) define o comportamento do grafo após a moderação: se o roteador indicar que a resposta precisa ser refeita, o fluxo retorna ao \"assistente\"; se estiver adequada, o processo é encerrado (END)."
      ],
      "metadata": {
        "id": "UkRFV8PjFCho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compilação do Grafo\n",
        "Esse trecho adiciona memória ao grafo e o prepara para execução. O MemorySaver() cria um checkpointer que salva o estado da conversa, permitindo retomar o fluxo ou manter o histórico entre interações. Em seguida, workflow.compile(checkpointer=checkpointer) compila o grafo com essa funcionalidade, gerando o aplicativo (app) pronto para execução com suporte à memória."
      ],
      "metadata": {
        "id": "v8qSMFcJFnKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para incluir memória inclua checkpointer no compile.\n",
        "checkpointer = MemorySaver()\n",
        "app = workflow.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "mfXL2FCbFuam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execução do Agente de Astronomia\n",
        "Agora, finalmente, podemos executar nosso agente de astronomia e fazer perguntas a ele!\n"
      ],
      "metadata": {
        "id": "h6fXbXSeGDd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_state = app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Olá! Quais as novidades astronômicas de hoje?\"),],\n",
        "        \"documento\": 'https://www.nasa.gov/news/recently-published/',\n",
        "        \"avaliacao\": \"\",\n",
        "        \"feedback\": \"\"\n",
        "     },\n",
        "    config={\"configurable\": {\"api_key\": API_KEY, \"thread_id\": 42}}\n",
        ")\n",
        "\n",
        "# Estado final do grafo\n",
        "print(final_state)\n",
        "\n",
        "# Saída do print\n",
        "{'messages': [HumanMessage(content='Olá! Quais as novidades astronômicas de hoje, dia 24/10/2025?', additional_kwargs={}, response_metadata={}),\n",
        "\n",
        "              AIMessage(content='''Uma das novidades astronômicas de hoje, dia 24/10/2025, é a chuva de meteoros Orionid, que está brilhando intensamente no céu.\n",
        "              Esse evento é um dos destaques da astronomia para o mês de outubro.''',\n",
        "              additional_kwargs={}, response_metadata={})],\n",
        "\n",
        " 'documento': 'https://www.nasa.gov/news/recently-published/',\n",
        "\n",
        " 'avaliacao': 'sim',\n",
        "\n",
        " 'feedback': 'Você não possui feedback.',\n",
        "}\n"
      ],
      "metadata": {
        "id": "TausbB_kGbfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse trecho executa o grafo completo. A função app.invoke() inicia o fluxo passando um estado inicial com a mensagem do usuário e o link do documento usado pelo RAG . O parâmetro config define informações adicionais, como a chave da API e o identificador da conversa. Ao final da execução, o resultado, que contém todas as mensagens e dados gerados pelo fluxo, é armazenado em final_state e exibido com print(final_state), mostrando o estado final do grafo após todo o processamento.\n",
        "\n",
        "Neste passo a passo, vimos que é totalmente possível transformar uma ideia em um agente LLM funcional, capaz de buscar informações, compreendê-las e gerar respostas de forma autônoma. Foi construído um fluxo lógico e integrada uma fonte de conteúdo que permitiu ao agente aprender a revisar o que diz antes de responder. O mais interessante é que esse esqueleto pode ser adaptado para qualquer tema: basta trocar a fonte, adicionar novas “habilidades” e você terá um assistente inteligente moldado ao seu próprio universo!"
      ],
      "metadata": {
        "id": "y9_Agg0XGiEV"
      }
    }
  ]
}